{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Documents used:\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html\n",
    "    - https://machinelearningmastery.com/nearest-shrunken-centroids-with-python/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'coroutine' from 'asyncio' (c:\\Users\\Daito\\miniconda3\\envs\\ML\\Lib\\asyncio\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Daito\\OneDrive\\Bureau\\CapProjetRecherche\\code\\ML_model_implementation\\nearest_class_mean.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daito/OneDrive/Bureau/CapProjetRecherche/code/ML_model_implementation/nearest_class_mean.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m pair_confusion_matrix\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daito/OneDrive/Bureau/CapProjetRecherche/code/ML_model_implementation/nearest_class_mean.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Daito/OneDrive/Bureau/CapProjetRecherche/code/ML_model_implementation/nearest_class_mean.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmemory_profiler\u001b[39;00m \u001b[39mimport\u001b[39;00m memory_usage\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daito/OneDrive/Bureau/CapProjetRecherche/code/ML_model_implementation/nearest_class_mean.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daito/OneDrive/Bureau/CapProjetRecherche/code/ML_model_implementation/nearest_class_mean.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daito\\miniconda3\\envs\\ML\\Lib\\site-packages\\memory_profiler.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.58.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      8\u001b[0m _CMD_USAGE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython -m memory_profiler script_file.py\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39masyncio\u001b[39;00m \u001b[39mimport\u001b[39;00m coroutine, iscoroutinefunction\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcontextlib\u001b[39;00m \u001b[39mimport\u001b[39;00m contextmanager\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial, wraps\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'coroutine' from 'asyncio' (c:\\Users\\Daito\\miniconda3\\envs\\ML\\Lib\\asyncio\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,RandomizedSearchCV,RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader():\n",
    "    def __init__(self):\n",
    "        self.df=None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.numbers_of_classes = None\n",
    "        self.classes=None\n",
    "\n",
    "    def read_dataset(self,file_path,separator=',',class_path=None):\n",
    "        df = pd.read_csv(file_path, sep=separator)\n",
    "        self.X = df.copy()\n",
    "        if class_path!=None:\n",
    "            df['class'] = pd.read_csv(class_path)\n",
    "            self.y = df['class']\n",
    "            self.classes=df['class'].unique()\n",
    "            self.numbers_of_classes = len(self.classes)\n",
    "        self.df=df\n",
    "\n",
    "\n",
    "    def normalize(self,features_to_normalize=None): #features_to_normalize is a list of index\n",
    "        if features_to_normalize!=None:\n",
    "            return normalize(self.X[:,features_to_normalize[0]:features_to_normalize[1]])\n",
    "\n",
    "    def select_features(self,features): #features is a list of features [feature1,feature2,...] or [:156]\n",
    "        self.X=self.df.iloc[features]\n",
    "    \n",
    "    def select_classes(self,classes): #classes is a list of classes [class1,class2,...] with len(classes) = len of dataset\n",
    "        self.y = classes\n",
    "        self.classes=np.unique(classes)\n",
    "        self.numbers_of_classes = len(self.classes)\n",
    "        \n",
    "    def split_dataset(self,test_size=0.2): #slit dataset into train and test\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size)\n",
    "    \n",
    "    def split_dataset_class(self,class_to_group): #split dataset into train and test based on class\n",
    "        # Initialize empty lists to store merged sets\n",
    "        X_train_merged, X_test_merged, y_train_merged, y_test_merged = [], [], [], []\n",
    "        for i in class_to_group.values():\n",
    "            df_temp=self.df[self.df['class'].isin(i)]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df_temp.iloc[:,:-1], df_temp['class'], test_size=0.2)\n",
    "            X_train_merged.append(X_train)\n",
    "            X_test_merged.append(X_test)\n",
    "            y_train_merged.append(y_train)\n",
    "            y_test_merged.append(y_test)\n",
    "\n",
    "        # Merge sets\n",
    "        return X_train_merged, X_test_merged, y_train_merged, y_test_merged\n",
    "    \n",
    "    def split_dataset_data(self,n): #split dataset by data size\n",
    "        X_train_merged,X_test_merged,y_train_merged,y_test_merged=[],[],[],[]\n",
    "        df_copy=self.df.copy()\n",
    "        df_copy.pop('class')\n",
    "        skf=StratifiedKFold(n_splits=n,shuffle=False)\n",
    "        skf.get_n_splits(self.X_train,self.y_train)\n",
    "        for i,(train_index, test_index) in enumerate(skf.split(self.X, self.y)):\n",
    "            X_train_fold=df_copy.iloc[train_index]\n",
    "            Y_train_fold=self.y[train_index]\n",
    "            X_train_merged.append(X_train_fold)\n",
    "            y_train_merged.append(Y_train_fold)\n",
    "\n",
    "            X_test_fold=df_copy.iloc[test_index]\n",
    "            Y_test_fold=self.y[test_index]\n",
    "            X_test_merged.append(X_test_fold)\n",
    "            y_test_merged.append(Y_test_fold)\n",
    "        \n",
    "        return X_train_merged,y_train_merged,X_test_merged,y_test_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader():\n",
    "    def __init__(self):\n",
    "        self.model=None\n",
    "        self.initial_model=None\n",
    "        self.optimizer_model=None\n",
    "        self.optimal_params=None\n",
    "\n",
    "    def set_optimal_params(self,optimal_params):\n",
    "        self.optimal_params=optimal_params\n",
    "\n",
    "    def set_model(self,model):\n",
    "        self.model=model\n",
    "        self.initial_model=model\n",
    "\n",
    "    def reset_model(self):\n",
    "        self.model=self.initial_model\n",
    "\n",
    "    def optimize(self,X_train,y_train,cv=5,scoring='accuracy',n_iter=10):\n",
    "        self.optimizer_model = RandomizedSearchCV(self.model,self.optimal_params,cv=cv,scoring=scoring,n_iter=n_iter)\n",
    "        self.optimizer_model.fit(X_train,y_train)\n",
    "        self.optimal_params = self.optimizer_model.best_params_\n",
    "\n",
    "    def fit_train(self,X_train,y_train):\n",
    "        self.model.fit(X_train,y_train)\n",
    "\n",
    "    def partial_fit_train(self,X_train,y_train,classes): # IF model compatible with partial_fit\n",
    "        mem, res = memory_usage(( self.model.partial_fit, (X_train,y_train,classes), {}), retval=True)\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        return self.model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data \n",
    "loader = DatasetLoader()\n",
    "loader.read_dataset('partial_database.csv',class_path='labelsDefault.txt',separator=',')\n",
    "loader.split_dataset()\n",
    "\n",
    "#Normalize data, if not already normalized\n",
    "print(np.shape(loader.X_train),np.shape(loader.X_test),np.shape(loader.y_train),np.shape(loader.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.579\n",
      "Config: {'shrink_threshold': 0.06343448547094324}\n"
     ]
    }
   ],
   "source": [
    "#Get model\n",
    "model = ModelLoader()\n",
    "model.set_model(NearestCentroid())\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=22, n_repeats=3, random_state=1)\n",
    "\n",
    "#Optimization \n",
    "param = {{'shrink_threshold':np.random.normal(0.06, 0.01,1000)}}\n",
    "model.set_optimal_params(param)\n",
    "\n",
    "model.optimize(loader.X_train,loader.y_train,cv=cv,scoring='accuracy',n_iter=10)\n",
    "\n",
    "#Train model with optimal params\n",
    "model.set_model(NearestCentroid(**model.optimal_params))\n",
    "model.fit_train(loader.X_train,loader.y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = model.predict(loader.X_test)\n",
    "\n",
    "#Score\n",
    "print(model.score(loader.X_test,loader.y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
